{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach to the existing event loop when using jupyter notebooks\n",
    "import nest_asyncio\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Chroma using Sentence Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    SentenceTransformersTokenTextSplitter,\n",
    ")\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from pypdf import PdfReader\n",
    "pdf_path = os.getenv(\"FILE_PATH_ENG\")\n",
    "\n",
    "CHROMA_PERSISTENT_DIR = \"../../data/chroma\"\n",
    "CHROMA_COLLECTION_NAME = \"book_eng_mini_l6\"  # 384\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Rec char Splitter\n",
    "character_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    ")\n",
    "\n",
    "# sentence token splitter\n",
    "token_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    chunk_overlap=0, tokens_per_chunk=256\n",
    ")\n",
    "\n",
    "pdf_bytes = None\n",
    "with open(pdf_path, \"rb\") as f:\n",
    "    pdf_bytes = f.read()\n",
    "\n",
    "doc = PdfReader(BytesIO(pdf_bytes))\n",
    "docs = []\n",
    "for page_num in range(len(doc.pages)):\n",
    "    pdf_page = doc.pages[page_num]\n",
    "    pdf_page_text = pdf_page.extract_text()\n",
    "\n",
    "    # skip empty pages\n",
    "    if not pdf_page_text:\n",
    "        continue\n",
    "\n",
    "    # split text\n",
    "    character_split_texts = character_splitter.split_text(pdf_page_text)\n",
    "\n",
    "    token_split_texts = []\n",
    "    for text in character_split_texts:\n",
    "        token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "    # create metadata from token split\n",
    "    page_nr = int(page_num + 1)\n",
    "\n",
    "    # set metadata for each split\n",
    "    metadatas = [\n",
    "        {\"source\": pdf_path, \"page\": page_nr} for _ in token_split_texts\n",
    "    ]\n",
    "\n",
    "    # convert to document\n",
    "    documents = character_splitter.create_documents(\n",
    "        texts=token_split_texts, metadatas=metadatas\n",
    "    )\n",
    "\n",
    "    docs.extend(documents)\n",
    "\n",
    "db = Chroma.from_documents(\n",
    "    collection_name=CHROMA_COLLECTION_NAME,\n",
    "    documents=docs,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=CHROMA_PERSISTENT_DIR,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Chroma Db with SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "\n",
    "\n",
    "CHROMA_PERSISTENT_DIR = \"../../data/chroma\"\n",
    "CHROMA_COLLECTION_NAME = \"book_eng_mini_l6\"  # 384\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "vectorstore = Chroma(\n",
    "            collection_name=CHROMA_COLLECTION_NAME,\n",
    "            persist_directory=CHROMA_PERSISTENT_DIR,\n",
    "           embedding_function=embedding_function,\n",
    "        )\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": 3, \"score_threshold\": 0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The name of the drummer is Eugene.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing it out\n",
    "question = \"What's the name of the drummer?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = [\n",
    "    \"What is name of the forrest?\",\n",
    "    \"Who is having a birthday?\",\n",
    "]\n",
    "\n",
    "eval_answers = [\n",
    "    \"The name of the forrest is Starry Sky\",\n",
    "    \"It's Fred birthday\",    \n",
    "]\n",
    "\n",
    "examples = [\n",
    "    {\"query\": q, \"ground_truths\": [eval_answers[i]]}\n",
    "    for i, q in enumerate(eval_questions)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain.invoke({\"query\": eval_questions[1]})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain.invoke(examples[0])\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Ragas Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'What is name of the forrest?',\n",
       "  'ground_truths': ['The name of the forrest is Starry Sky']},\n",
       " {'query': 'Who is having a birthday?',\n",
       "  'ground_truths': [\"It's Fred birthday\"]}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"What's the name of the drummer?\",\n",
       " 'result': 'The name of the drummer is Eugene.',\n",
       " 'source_documents': [Document(page_content='nowadays mr. m is the most famous manager in the undergrowth. he is the one who made forest band to play all over the world. he met fred a long time ago. at that time, he was working a lot and needed to relax a bit. it was then that he discovered the magical \" nap in nature \" course organized by fred. since that day, they have became great friends, sharing moments of music, laughter, and joy.', metadata={'page': 27, 'source': 'C:\\\\Users\\\\nttLu\\\\OneDrive\\\\Illustrated Book\\\\Midjourney\\\\The Adventure of Starry Sky V3.pdf'}),\n",
       "  Document(page_content='\" hey, wait a minute... what about the instruments? \" says fred. \" we need the piano and backing tracks for vj, the bass for flora, the guitar for stella, and the drums for eugene... and of course, a microphone for me. how can we get all this before tonight? \"', metadata={'page': 39, 'source': 'C:\\\\Users\\\\nttLu\\\\OneDrive\\\\Illustrated Book\\\\Midjourney\\\\The Adventure of Starry Sky V3.pdf'}),\n",
       "  Document(page_content='his great friend alfred von jurgen, known as vj, is among the very first to receive the invitation. vj is a hazel bear descended from the noble bear families of the north. he has explored many lands and learned ancient bear dialects. he is the most technologically advanced bear in his family because he studied at a. i. t., the famous animal institute of technology. in this school, he learned the secrets of the compu ter world and electronic music.', metadata={'page': 11, 'source': 'C:\\\\Users\\\\nttLu\\\\OneDrive\\\\Illustrated Book\\\\Midjourney\\\\The Adventure of Starry Sky V3.pdf'})]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.langchain.evalchain import RagasEvaluatorChain\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "\n",
    "# create evaluation chains\n",
    "faithfulness_chain = RagasEvaluatorChain(metric=faithfulness)\n",
    "answer_rel_chain = RagasEvaluatorChain(metric=answer_relevancy)\n",
    "context_rel_chain = RagasEvaluatorChain(metric=context_precision)\n",
    "context_recall_chain = RagasEvaluatorChain(metric=context_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High faithfulness_score = exact consistency between source documents and answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = faithfulness_chain(result)\n",
    "eval_result[\"faithfulness_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = context_recall_chain(result)\n",
    "eval_result[\"context_recall_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval with Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = qa_chain.batch(examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = faithfulness_chain.evaluate(examples, predictions)\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using with Langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langsmith.utils import LangSmithError\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Ragas_with_LS\"\n",
    "\n",
    "try:\n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "    print(\"using existing dataset: \", dataset.name)\n",
    "except LangSmithError:\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name, description=\"Testing Ragas using LangSmith\"\n",
    "    )\n",
    "    for e in examples:\n",
    "        client.create_example(\n",
    "            inputs={\"query\": e[\"query\"]},\n",
    "            outputs={\"ground_truths\": e[\"ground_truths\"]},\n",
    "            dataset_id=dataset.id,\n",
    "        )\n",
    "\n",
    "    print(\"dataset created: \", dataset.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Caveat : define a factory function to create a new instance of the chain each time to prevent the reuse of state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factory function that return a new qa chain\n",
    "def create_qa_chain(return_context=True):\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=return_context,\n",
    "    )\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    custom_evaluators=[\n",
    "        faithfulness_chain,\n",
    "        answer_rel_chain,\n",
    "        context_rel_chain,\n",
    "        #context_recall_chain,\n",
    "    ],\n",
    "    prediction_key=\"result\",\n",
    ")\n",
    "\n",
    "result = run_on_dataset(\n",
    "    client,\n",
    "    dataset_name,\n",
    "    create_qa_chain,\n",
    "    evaluation=evaluation_config,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-articles-wuG-FtGG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
